# Intro to Prompt Engineering

Lecture: https://www.youtube.com/watch?v=dOxUroR57xs

## Notes

### Why
- Important for research, discoveries, and advancement
- Helps to test and evaluate the limitations of LLMs
- Enables all kinds of innovative applications

### First basic prompt: “The sky is blue”

- Model
- Temperature
- top-p

### Elements of a prompt

- Instructions — neutral / negative / positive
- Context
- Input data
- Output indicator (sentiment)

### Settings to keep in mind

- Get different results with prompts when using different settings
- Controlling how deterministic the model is when generating completion for prompts
- Temperature and top_p are two important parameters to keep in mind
- Generally, keep these low if you arel ooking for exact answers
- Keep them high if you are looking for more diverse responses

### Designing prompts for different tasks

Categories:
- Text Summarization
- Question Answering
- Text Classification
- Role Playing
- Code Generation
- Reasoning

### Prompt Examples

- Text Summarization: "Explain the above in one sentence" (instruction; concise or precise way)
- Question Answering: Instruction => Context => Question.
- Text Classification: "Classify the text into neutral, negative, or positive."
- Role Playing: "Behave in a certain way - let the tone be technical and scientific."
- Code Generation: "Write a function that takes in a number and returns the square of that number."
- Reasoning: "Explain why the above is true."
    - If these AIs can go about reasoning, this is a huge step forward in combining seemingly disparate contexts and drawing a new conclusion from them that isn't yet written.

### Prompt Engineering Techniques

- Few shot prompts: allows us to provide **exemplars** in prompts to seter the model towards better performance. Give it an idea of what the task is about, and follow the pattern of the exemplars. It's like how I learn what to do and apply in new situations (what have we done before, and given those patterns, how can I apply it here?)
- Chain of thought prompting: Prompting can be further improved by instructing the model to reason about the task when responding.
  - Very useful tor tasks that require reasoning
  - You can combine it with few-shot prompting to get better results
  - You can also do zero-shot CoT where exemplats are not available
    - Involves adding "Let's think step by step" to the original prompt
      - Large Language Models are Zero-shot Reasoners (!!)
- Self consistency: aims to improve on the naive greedy decoding used in chain-of-thought prompting
  - The idea is to sample multiple, diverse reasoning paths through few-shot CoT, and use the generations to select the most consistent answer
  - This helps to boost the performance of CoT prompting on thsks involving arithmetic and common-sense reasoning
- Knowledge generation prompting
  - This technique involves using additional knowledge provided as part of the context to improve results on complex tasksssuch as commonsense reasoning
  - The knowledge used in the context is generated by a model and used in the prompt to make a prediction; highest-confidence prediction is used
  - Example: The first step is to generate knowledge. Give it a prompt template. Then, give it a context. The context is the knowledge that the model generates. Then, give it a question. The model will answer the question based on the knowledge that it generated. Explain and answer is the key here.
- ReAct = Reason + Act
  - Framework where LLMs are used to generate both reasoning traces and task-specific actions in an interleaved manner
  - Generating reasoning traces allow the model to incude, track, and update action plans, and even handle exceptions
  - The action step allows to interface with and gather information from external sources such as knowledge bases or environments
  - In other words: this allows LLMs to interact with external tools to retrieve additional information that leads to more reliable and factual responses.
- Program-aided language model (PAL)
  - Chain-of-thought prompting is a good example of how to steer models to perform better at complex reasoning tasks
    - However, sometimes CoT is not enough as it depends only on the generated text from the model
  - Program-aided langauge models (PAL) uses an LLM to read problems and generate programs as the intermediate reasoning steps; it offloads the solution step to a runtime such as Python interpreter


### Zero-shot CoT Example

```python
prompt = """
I went to the market and bought 10 apples.
I gave 2 apples to the neighbor and 2 to the repairman.
I then went and bought 5 more apples and ate 1.
How many apples did I remain with?

Let's think step by step."""

response = get_completion(params, prompt)
IPython.display.Markdown(response.choices[0].text)
```

- Reduce the stereotype bias by provide a very precise prompt


### Questions

- On classification: are there other things that you can classify on besides neutral, negative, and positive? I need five examples.
- On `Data-augmented generation`: how can I get started? Is there a tool or playground for this?

